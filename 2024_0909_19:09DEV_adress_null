import scrapy
from scrapy.http.response.html import HtmlResponse
from urllib.parse import urljoin
import pandas as pd
from datetime import datetime
import os

class SuumoSpider(scrapy.Spider):
    name = "suumo"
    origin = "https://suumo.jp"
    allowed_domains = ["suumo.jp"]
    start_urls = [
        "https://suumo.jp/jj/chintai/ichiran/FR301FC005/?ar=030&bs=040&ta=13&sc=13113&cb=0.0&ct=8.0&mb=0&mt=9999999&et=20&cn=5&shkr1=03&shkr2=03&shkr3=03&shkr4=03&sngz=&po1=25&po2=99&pc=50"
    ]
    
    properties = []  # データを保存するリスト

    def parse(self, response: HtmlResponse):
        self.handle_response(response)

        # ページネーション処理
        paginators = response.css(".pagination-parts")
        if paginators:
            last_paginator = paginators[-1]
            if last_paginator.css("::text").get() == "次へ":
                next_url = last_paginator.css("a").attrib.get("href")
                if next_url:
                    yield scrapy.Request(url=urljoin(self.origin, next_url))

    def handle_response(self, response: HtmlResponse):
        for row in response.css("div.property"):
            title = row.css("h2.property_inner-title a::text").get(default='').strip()
            rent_price = row.css("div.detailbox-property-point::text").get(default='').strip()
            
            property_table = row.css("div.detailbox > div.detailbox-property > table")
            plan_of_house = property_table.css("td.detailbox-property--col3 > div:nth-child(1)::text").get(default='').strip()
            exclusive_area = property_table.css("td.detailbox-property--col3 > div:nth-child(2)::text").get(default='').strip()
            
            detail_texts = row.css("div.detailbox-note div.detailnote-box div::text").getall()
            detail_texts = [t.strip() for t in detail_texts if t.strip() != ""]
            detail = "\n".join(detail_texts).strip()

            # 住所の取得
            address = self.extract_address(row)

            # スクレイピング結果を一時的にリストに保存
            self.properties.append({
                "Title": title,
                "Rent Price": rent_price,
                "Plan of House": plan_of_house,
                "Exclusive Area": exclusive_area,
                "Detail": detail,
                "Address": address,  # 住所を追加
            })

    def extract_address(self, row):
        """
        住所を取得するためのセレクタを試行する
        """
        # 住所を取得するセレクタのリスト
        address_selectors = [
            "td.detailbox-property-col::text",  # 住所が含まれる<td>要素
            "div.property-body-element td.detailbox-property-col::text",  # 他の場所にある場合
            "td.detailbox-property-col.detailbox-property--col3 + td.detailbox-property-col::text",  # 隣接する場合
        ]

        for selector in address_selectors:
            address = row.css(selector).get(default='').strip()
            if address:
                return address

        return ''  # 住所が見つからない場合

    def close(self, reason):
        """
        スパイダーが全ての処理を完了した際に呼び出される
        """
        self.save_to_excel()

    def save_to_excel(self):
        try:
            # pandasを使ってデータをExcelに変換
            df = pd.DataFrame(self.properties)
            
            # 現在の日付と時間をファイル名に組み込む
            current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            file_name = f"suumo_data_{current_time}.xlsx"

            # ファイルを保存するパスを指定（環境に合わせて変更）
            file_path = os.path.join(os.path.expanduser('~'), 'Downloads', file_name)
            df.to_excel(file_path, index=False)

            self.log(f"Excel file saved: {file_path}")
        except Exception as e:
            self.log(f"Error saving Excel file: {e}", level=scrapy.logging.ERROR)
