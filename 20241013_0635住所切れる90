import scrapy
from scrapy.http.response.html import HtmlResponse
from urllib.parse import urljoin
import pandas as pd
from datetime import datetime
import os


class SuumoSpider(scrapy.Spider):
    name = "suumo"
    origin = "https://suumo.jp"
    allowed_domains = ["suumo.jp"]
    start_urls = [
        "https://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ra=014&ae=01251&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2=&ek=012540940&rn=0125"
    ]

    properties = []  # 物件情報を保存するためのリスト
    batch_size = 100  # 100件ごとに保存
    file_count = 1  # 保存ファイルのカウンタ

    def parse(self, response: HtmlResponse):
        """
        各ページの物件データを処理し、次のページがあればリクエストを送ります。
        """
        # 現在のページの物件データを処理します
        self.handle_response(response)

        # 「次へ」ボタンのリンクを取得
        next_page = response.css('p.pagination-parts a::attr(href)').get()

        if next_page:
            # URLを正規化して次のページにリクエスト
            next_page_url = urljoin(self.origin, next_page)
            self.log(f"次のページへ移動します: {next_page_url}")
            yield scrapy.Request(url=next_page_url, callback=self.parse)
        else:
            self.log("次のページはありません。スクレイピングを終了します。")

    def handle_response(self, response: HtmlResponse):
        """
        物件データを処理するためのメソッドです。
        各物件の情報を抽出してリストに保存します。
        """
        for property in response.css("div.cassetteitem"):
            # 1. 'name': 物件名（マンション名）を取得
            name = property.css('.cassetteitem_content-title::text').get(default='').strip()

            # 2. 'address': 物件の住所を取得
            address = property.css('li.cassetteitem_detail-col1::text').get(default='').strip()

            # 3. 'stations': 物件の最寄り駅情報をリスト形式で取得し、カンマ区切りで結合
            stations = property.css('.cassetteitem_detail-col2 .cassetteitem_detail-text::text').getall()
            stations = ', '.join([station.strip() for station in stations])

            # 4. 'year_built': 物件の築年数を取得
            year_built = property.css('li.cassetteitem_detail-col3 > div:nth-child(1)::text').get(default='').strip()
            if '築' in year_built:
                year_built = year_built.replace('築', '').replace('年', '').strip()
            else:
                year_built = '不明'

            # 5. 'floors': 物件の階数（地上何階建てか）を取得
            floors = property.css('li.cassetteitem_detail-col3 > div:nth-child(2)::text').re_first(r'(\d+)階建')
            if not floors:
                floors = '不明'

            # データをリストに追加
            self.properties.append({
                "name": name,
                "address": address,
                "stations": stations,
                "year_built": year_built,
                "floors": floors,
            })

            # 100件ごとにデータを保存
            if len(self.properties) >= self.batch_size:
                self.save_to_excel()

    def save_to_excel(self):
        """
        100件ごとにExcelファイルとして保存する関数
        """
        # pandas でデータフレームに変換
        df = pd.DataFrame(self.properties)

        # 現在の日付を取得しファイル名を作成
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_name = f"suumo_data_batch_{self.file_count}_{current_time}.xlsx"

        # ダウンロードフォルダの取得
        download_dir = os.path.join(os.path.expanduser("~"), "Downloads")
        if not os.path.exists(download_dir):
            # ダウンロードフォルダが存在しない場合はホームディレクトリに保存
            file_path = os.path.join(os.path.expanduser("~"), file_name)
        else:
            # ダウンロードフォルダが存在する場合はその中に保存
            file_path = os.path.join(download_dir, file_name)

        # Excel ファイルを保存
        df.to_excel(file_path, index=False)

        # ログを出力
        self.log(f"{len(self.properties)}件のデータが保存されました: {file_path}")

        # ファイル保存後、ファイルカウンタをインクリメントし、リストをクリア
        self.file_count += 1
        self.properties.clear()  # リストをクリアして次の100件を収集

    def close(self, reason):
        """
        スパイダーの全ての処理が終了した時に呼ばれるメソッドです。
        残っているデータを保存します。
        """
        if self.properties:
            self.save_to_excel()  # 残りのデータを保存
