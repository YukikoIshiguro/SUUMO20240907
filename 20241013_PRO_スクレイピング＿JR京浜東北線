import scrapy
from scrapy.http.response.html import HtmlResponse
from urllib.parse import urljoin
import pandas as pd
from datetime import datetime
import re
import os  # 追加

class SuumoSpider(scrapy.Spider):
    name = "suumo"
    origin = "https://suumo.jp"
    allowed_domains = ["suumo.jp"]
    start_urls = [
        "https://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ra=014&ae=01251&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2=&ek=012540940&rn=0125"
    ]

    properties = []  # 物件情報を保存するためのリスト

    def parse(self, response: HtmlResponse):
        """
        スパイダーが最初に呼ばれるメソッドです。
        各ページの物件データを処理し、次のページがあればリクエストを送ります。
        """
        # 現在のページの物件データを処理します
        self.handle_response(response)

        # 次のページがあるかを確認
        next_page = response.css("a.pagination-next::attr(href)").get()
        if next_page:
            yield scrapy.Request(url=urljoin(self.origin, next_page), callback=self.parse)

    def handle_response(self, response: HtmlResponse):
        """
        物件データを処理するためのメソッドです。
        各物件の情報を抽出してリストに保存します。
        """
        for property in response.css("div.cassetteitem"):
            # 1. 'name': 物件名（マンション名）を取得
            name = property.css('.cassetteitem_content-title::text').get(default='').strip()

            # 2. 'address': 物件の住所を取得
            address = property.css('li.cassetteitem_detail-col1::text').get(default='').strip()

            # 3. 'stations': 物件の最寄り駅情報をリスト形式で取得し、カンマ区切りで結合
            stations = property.css('.cassetteitem_detail-col2 .cassetteitem_detail-text::text').getall()
            stations = ', '.join([station.strip() for station in stations])

            # 4. 'year_built': 物件の築年数を取得
            year_built = property.css('li.cassetteitem_detail-col3 > div:nth-child(1)::text').get(default='').strip()
            if '築' in year_built:
                year_built = year_built.replace('築', '').replace('年', '').strip()
            else:
                year_built = '不明'

            # 5. 'floors': 物件の階数（地上何階建てか）を取得
            floors = property.css('li.cassetteitem_detail-col3 > div:nth-child(2)::text').re_first(r'(\d+)階建')
            if not floors:
                floors = '不明'

            # データをリストに追加
            self.properties.append({
                "name": name,
                "address": address,
                "stations": stations,
                "year_built": year_built,
                "floors": floors,
            })

    def close(self, reason):
        """
        スパイダーの全ての処理が終了した時に呼ばれるメソッドです。
        データを Excel ファイルとして保存します。
        """
        self.save_to_excel()

    def save_to_excel(self):
        """
        スクレイピングしたデータを Excel ファイルとして保存します。
        """
        # pandas でデータフレームに変換
        df = pd.DataFrame(self.properties)

        # 現在の日付を取得しファイル名を作成
        current_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        file_name = f"suumo_data_{current_time}.xlsx"

        # ダウンロードフォルダの取得
        download_dir = os.path.join(os.path.expanduser("~"), "Downloads")
        if not os.path.exists(download_dir):
            # ダウンロードフォルダが存在しない場合はホームディレクトリに保存
            file_path = os.path.join(os.path.expanduser("~"), file_name)
        else:
            # ダウンロードフォルダが存在する場合はその中に保存
            file_path = os.path.join(download_dir, file_name)

        # Excel ファイルを保存
        df.to_excel(file_path, index=False)

        # ファイル保存をログに出力
        self.log(f"Excelファイルが保存されました: {file_path}")
